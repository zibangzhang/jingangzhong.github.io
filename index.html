<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Smart Computational Imaging Laboratory @ Jinan University, Guangzhou China</title>
<style type="text/css">
body table {
	color: #000;
	text-align: center;
	font-family: Georgia;
	font-size: 24px;
}
body table tr td {
	color: #333333;
	font-size: 24px;
}
body table {
	font-family: Arial, Helvetica, sans-serif;
}
body table {
	font-size: 24px;
}
body table {
	color: #000;
}
body table {
	color: #000;
	font-size: 24px;
}
MenuDiv
{
	box-shadow: 20px 20px 30px gray;
	z-index: 2020;
}
Div
{
	box-shadow：0px 0px 0px red;
}
.Header {
	font-family: "Trebuchet MS", Arial, Helvetica, sans-serif;
	font-weight: bold;
	font-size: 58px;
}
.feet {
	color: #FFF;
	font-family: Georgia;
	text-align: left;
}
a:link {
	color: #000;
	text-decoration: none;
}
a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: none;
}
a:active {
	text-decoration: none;
}
</style>
</head>

<body>
<div>
<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#FFFFFF">
  <tr bgcolor="#B1040E">
    <td height="10" colspan=2"></td>
  </tr>
  <tr>
    <td width="5%" bgcolor="#B1040E"></td>
    <td valign="baseline" bgcolor="#B1040E" style="text-align: left; font-size: 28px; font-family: Georgia, 'Times New Roman', Times, serif; color: #FFFFFF;">Smart Computational Imaging Laboratory<span style="font-size: 18px; text-align: left; font-family: Georgia, 'Times New Roman', Times, serif;"> <span style="font-size: 30px">|</span> Jinan University, Guangzhou, China</span></td>
  </tr>
  <tr bgcolor="#B1040E">
    <td height="10" colspan="4"></td>
  </tr>
</table>
</div>
<div style = "MenuDiv"><table width="100%" border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td width="20%" height="5" bgcolor="#F2F1EB"></td>
    <td width="15%" height="15" bgcolor="#F2F1EB"></td>
    <td width="15%" height="15" bgcolor="#F2F1EB"></td>
    <td width="15%" height="15" bgcolor="#F2F1EB"></td>
    <td width="15%" height="15" bgcolor="#F2F1EB"></td>
    <td width="20%" height="15" bgcolor="#F2F1EB"></td>
  </tr>
  <tr>
    <td width="20%" height="5" bgcolor="#F2F1EB">&nbsp;</td>
    <td width="15%" height="5" bgcolor="#F2F1EB" style="color: #000000"><a href="index.html" style="color: #B1040E">News</a></td>
    <td width="15%" height="5" bgcolor="#F2F1EB" style="color: #B1040E"><a href="members.html">Members</a></td>
    <td width="15%" height="5" bgcolor="#F2F1EB"><a href="publications.html">Publications</a></td>
    <td width="15%" height="15" bgcolor="#F2F1EB"><a href="about.html">About</a></td>
    <td width="20%" height="5" bgcolor="#F2F1EB">&nbsp;</td>
  </tr>
  <tr>
    <td width="20%" height="5" bgcolor="#F2F1EB"></td>
    <td width="15%" height="15" bgcolor="#F2F1EB"></td>
    <td width="15%" height="15" bgcolor="#F2F1EB"></td>
    <td width="15%" height="15" bgcolor="#F2F1EB"></td>
    <td width="15%" height="15" bgcolor="#F2F1EB"></td>
    <td width="20%" height="15" bgcolor="#F2F1EB"></td>
  </tr>
</table>
</div>
<div>
  <table width="100%" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td class="Header">News</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
    </tr>

  </table>
</div>
<div>
  <table width="100%" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td>&nbsp;</td>
      <td align="left"><strong>2024/2/14</strong></td>
      <td>&nbsp;</td>
    </tr>
	<tr>
      <td>&nbsp;</td>
      <td align="left"><span style="text-align: left; font-size: 19px">We reported a real-time single-pixel imaging method that can adaptively balance the spatial resolution, temporal resolution, and signal-to-noise ratio of the imaging system according to the changes in the target scene. When scene changes are detected, the dynamic imaging mode will be activated. The temporal resolution will be given high priority and real-time single-pixel imaging will be conducted at a video frame rate (30 frames/s) to visualize the object motion. When no scene changes are detected, the static imaging mode will be activated. The spatial resolution and the signal-to-noise ratio will be progressively built up to resolve fine structures and to improve image quality. The proposed method not only adds practicability to single-pixel imaging, but also generates a new, to the best of our knowledge, insight in data redundancy reduction and information capacity improvement for other computational imaging schemes.<span style="text-align: left; font-size: 19px;"> This work was published in OPTICS LETTERS.</span></span></td>
      <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left">&nbsp;</td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left"><strong>2024/1/24</strong></td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left"><span style="text-align: left; font-size: 19px">We reported an image-free classification approach based on single-pixel measuring at the same spatial position of different rotation cycles to reduce motion blur. The proposed approach allows classifying rotating objects with fluctuating rotation periods to better meet the actual application conditions. We verify the proposed method by classifying the digits placed on a rotating disk. In our experiments, when digits rotate at around 960.9 revolutions per minute, corresponding to 10.06 m/s, the classification accuracy reaches 95.9%. In theory, the allowable speed is determined only by the sampling rate of the single-pixel measurements, which can allow for higher speeds than experimentally achieved. The proposed image-free classification method provides a promising way of monitoring high-speed rotating objects in the engineering field.<span style="text-align: left; font-size: 19px;"> This work was published in APPLIED PHYSICS LETTERS.</span></span></td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left">&nbsp;</td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left"><strong>2023/11/15</strong></td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left"><span style="text-align: left; font-size: 19px">We reported 
	    a novel autofocusing method which allows for single-pixel microscopy. The key to the proposed method is to determine the optimal focal position from the magnitude of a small number of Fourier coefficients, which is both data- and computation-efficient. Different from our last proposed autofocus method, grating is not required and the setup is simplified, and it is also applicable to other single-pixel imaging approaches.<span style="text-align: left; font-size: 19px;"> This work was published in OPTICS LETTERS.</span></span></td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left">&nbsp;</td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left"><strong>2023/8/22</strong></td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left"><span style="text-align: left; font-size: 19px">We develop contrast-enhanced, single-shot LED array microscopy based on the Fourier ptychographic algorithm and deep learning. The sample to be imaged is under illumination by all LEDs of the array simultaneously. The image captured is fed to several trained convolutional neural networks to generate the same number of images that are required by the Fourier ptychographic algorithm. We experimentally present that the image contrast of the final reconstruction is remarkably improved in comparison with the image captured. The proposed method can also produce chromatic-aberration-free results, even when an objective without aberration correction is used. We believe the method might provide live sample imaging with a low-cost approach. <span style="text-align: left; font-size: 19px;">This work was published in JOURNAL OF MICROSCOPY.</span></span></td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left">&nbsp;</td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left"><strong>2023/4/3</strong></td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
	  <td>&nbsp;</td>
	  <td align="left"><span style="text-align: left; font-size: 19px">We reported 
		a novel autofocusing method which allows a digital projector to find the 
		focus position without using a camera. Instead, the method works by 
		repeatedly projecting three fringe patterns and determining the focus 
		position rapidly with a photodiode. Inspired by the method, autofocusing 
		Fourier single-pixel imaging is achieved. To our best knowledge, it is 
		for the first time that single-pixel imaging with autofocusing is 
	  reported. <span style="text-align: left; font-size: 19px;">This work was published in OPTICS LETTERS.</span></span></td>
	  <td>&nbsp;</td>
    </tr>
	<tr>
      <td>&nbsp;</td>
      <td align="left">&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left"><strong>2023/1/1</strong></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left"><span style="text-align: left; font-size: 19px;">We reported a single-pixel imaging method which allows for full-resolution, wide-field-of-view, and high-quality imaging. On the basis of fast Fourier single-pixel imaging, we proposed a novel error diffusion kernel for pattern dithering and binarize the patterns with different dithering strategies. As such, one could obtain two or more single-pixel reconstructions. By synthesizing the reconstructed images, a high-quality image can be obtained. As experimentally demonstrated, two raw reconstructions are sufficient for the final high-quality reconstruction. In addition, the method allows one to make full use of the active area allowed by the spatial light modulator to achieve wide-field-of-view imaging. This work was published in OPTICS LETTERS.</span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left">&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left"><span style="text-align: left"><strong>2022/8/10</strong></span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left"><span style="text-align: left"> <span style="font-size: 12pt; font-family: &quot;微软雅黑&quot;"> </span></span><span style="text-align: left; font-size: 19px;">We proposed a light-field microscopy that can achieve full-resolution imaging. By extending the principle of dual photography from real space to Fourier space, the proposed method uses a spatial light modulator placed at the image plane as a virtual 2D detector to record the 2D spatial distribution of the image, and meanwhile, uses a real 2D detector placed at the Fourier plane of the image to record the angles of the light rays. The Fourier-spectrum signals recorded by each pixel of the real 2D detector can be used to reconstruct a perspective image through single-pixel imaging. Based on the perspective images reconstructed by different pixels, we experimentally demonstrated that the camera can digitally refocus on objects at different depths. The camera can achieve light-field imaging with full resolution and provide an extreme depth of field. The method provides a new idea for developing full-resolution light-field cameras. This work was published in PHOTONICS.</span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left">&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left"><span style="text-align: left"><strong>2022/4/15</strong></span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left"><span style="text-align: left"> <span style="font-size: 12pt; font-family: 微软雅黑"> 广东省科技创新大会在广州召开，会上颁发了2021年度广东省科学技术奖。课题组的《傅里叶单像素成像理论与方法》项目获得了自然科学奖二等奖。</span></span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left">&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left"><strong>2022/4/1</strong></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td style="text-align: left"><span style="text-align: left; font-size: 19px;">We proposed a method for illumiantion angle calibration. The key to the method is using a double-sided mask to encode the information about illumiantion angle. The mask is fabricated with a ring array pattern on its top surface and a disk array pattern on its bottom surface. The both patterns are concentric. However, there will be a relative shift between the two patterns, when the mask is under oblique illumination. Thus, we are able to retrieve the illumination angle information by measuring the amount of shift. This work was published in OPTICS LETTERS.</span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td align="left"><span style="text-align: left"><strong>2022/3/21</strong></span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td style="text-align: left"><span style="text-align: left; font-size: 19px;">We proposed a imaging-free method for fast-moving objects classification. The method is inspired by single-pixel imaging. The method uses orthogonal transform basis patterns to illuminate the target moving object and uses a single-pixel detector to collect the resulting light signals. By training a artifical neural network, the network is capable of classifying the target moving object from the input single-pixel measurements. This work was published in PHOTONICS.</span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td style="text-align: left"><strong>2022/2/25</strong></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td height="66">&nbsp;</td>
      <td style="text-align: left"><span style="text-align: left; font-size: 19px;">We proposed a new ringing artifacts elimination method to improve the imaging quality of Fourier single-pixel imaging. Wit the proposed method, Fourier single-pixel imaging is now able to achieve ringing artifacts free imaging for a dynamic scene. This work was published in OPTICS LETTERS.</span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td width="10%">&nbsp;</td>
      <td>&nbsp;</td>
      <td width="10%">&nbsp;</td>
    </tr>
	<tr>
      <td>&nbsp;</td>
      <td style="text-align: left"><strong>2021/12/7</strong></td>
      <td>&nbsp;</td>
    </tr>
	<tr>
      <td>&nbsp;</td>
      <td style="text-align: left">
		<span style="font-size: 12pt; font-family: 微软雅黑">
		课题组针对运动物体提出了一种光电混合神经网络，利用空间光调制和单像素探测对物体进行分类。实验证明，该方法可以长时间连续地对快速旋转的手写数字进行识别，其运动物体分类能力超过了人眼视觉。相关论文《基于光电混合神经网络的单像素快速运动物体分类（特邀）》发表在《红外与激光工程》。</span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td width="10%">&nbsp;</td>
      <td>&nbsp;</td>
      <td width="10%">&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td style="text-align: left"><strong>2021/8/9</strong></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td style="text-align: left"><span style="text-align: left; font-size: 19px;">We proposed a new sampling strategy, termed &quot;Gasussian random samping&quot;, for efficient Fourier single-pixel imaging. This sampling strategy can effecitvely reduce the number of measurements to achieve fast single-pixel imaging. The key to the strategy is to adopt density-varying sampling in the Fourier domain based on the importance of Fourier coefficients. This work was published in PHOTONICS.</span></td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td style="text-align: left">&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td width="10%">&nbsp;</td>
      <td style="text-align: left"><strong>2021/7/31</strong></td>
      <td width="10%">&nbsp;</td>
    </tr>
    <tr>
      <td width="10%">&nbsp;</td>
      <td style="text-align: left; font-size: 19px;">This site launched.</td>
      <td width="10%">&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td style="text-align: left; font-size: 19px;">&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
  </table>
</div>
<div>
  <table width="100%" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td bgcolor="#f4f4f4">&nbsp;</td>
    </tr>
    <tr>
      <td bgcolor="#f4f4f4" style="color: #000000; font-size: 19px;"><a href="mailto:scilab.jnu@gmail.com"><img src="images/email.png" width="20" height="15" /> Contact us</a></td>
    </tr>
    <tr>
      <td bgcolor="#f4f4f4">&nbsp;</td>
    </tr>
  </table>
</div>
<div>
  <table width="100%" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td height="20" colspan="4" bgcolor="#8c1515"></td>
    </tr>
    <tr>
      <td width="5%" height="20" bgcolor="#8c1515">&nbsp;</td>
      <td width="30%" height="20" valign="middle" bgcolor="#8c1515" style="line-height: 30px; font-family: Georgia"><p class="feet"><strong>Smart Computational Imaging</strong>      
      <td height="10" bgcolor="#8c1515" style="color: #FFF; font-size: 16px;">Mengminwei Science Building 404, JInan University, Huangpu Avenue West 601, Guangzhou, Guangdong, China</td>
      <td width="5%" height="10" bgcolor="#8c1515">&nbsp;</td>
    </tr>
    <tr>
      <td height="20" colspan="4" bgcolor="#8c1515"></td>
    </tr>
  </table>
</div>
<p>&nbsp;</p>
</body>
</html>
